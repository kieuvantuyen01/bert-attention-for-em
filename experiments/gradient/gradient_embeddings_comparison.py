from utils.general import get_dataset, get_sample
from utils.nlp import get_similar_word_pairs
import os
import pandas as pd
from pathlib import Path
from tqdm import trange, tqdm
from utils.test_utils import ConfCreator
from utils.grad_utils import load_saved_grads_data
import matplotlib.pyplot as plt
import pickle
import gensim
import numpy as np
import argparse
import distutils.util
from utils.data_collector import DM_USE_CASES

PROJECT_DIR = Path(__file__).parent.parent.parent
MODELS_DIR = os.path.join(PROJECT_DIR, 'results', 'models')
RESULTS_DIR = os.path.join(PROJECT_DIR, 'results', 'grads')
DATA_DIR = os.path.join(PROJECT_DIR, 'data')
FAST_TEXT_PATH = os.path.join(DATA_DIR, 'wiki-news-300d-1M.vec', 'wiki-news-300d-1M.vec')


def get_use_case_entity_pairs(conf, sampler_conf):
    dataset = get_dataset(conf)

    complete_sampler_conf = sampler_conf.copy()
    complete_sampler_conf['permute'] = conf['permute']
    sample = get_sample(dataset, complete_sampler_conf)

    return sample


def get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type, sim_metric, sim_thrs, sim_op_eq,
                                 sem_emb_model=None, continuous_res=False):

    grads = []
    for uc in conf['use_case']:
        print("\n\n", uc)
        uc_conf = conf.copy()
        uc_conf['use_case'] = uc

        # Get data
        encoded_dataset = get_use_case_entity_pairs(conf=uc_conf, sampler_conf=sampler_conf)

        # Get similar words
        pair_of_entities = [(row[0], row[1]) for row in encoded_dataset]
        similar_word_pair_map = get_similar_word_pairs(pair_of_entities=pair_of_entities, sim_type=sim_type,
                                                       metric=sim_metric, thrs=sim_thrs, op_eq=sim_op_eq,
                                                       sem_emb_model=sem_emb_model, continuous_res=continuous_res)

        # Get precomputed word gradients generated by an EM-fine-tuned BERT model
        uc_grad = load_saved_grads_data(uc, conf, sampler_conf, True, grad_conf, RESULTS_DIR)

        grad_data = []
        for g in uc_grad:
            grad_data.append({
                'left_words': [x[2:] for x in g['grad']['left']],
                'right_words': [x[2:] for x in g['grad']['right']],
                'left_grads': g['grad']['left_grad']['sum'],
                'right_grads': g['grad']['right_grad']['sum'],
            })

        # Get word pair embeddings and measure if they encode some similarity knowledge
        # Loop over the tested word pair similarity thresholds
        for thr in similar_word_pair_map:
            print(f"THR: {thr}")
            similar_word_pairs = similar_word_pair_map[thr]
            num_all_pairs = similar_word_pairs['num_all_pairs']
            sem_model_sims = [s for pair_sims in similar_word_pairs['sims'] for s in pair_sims]

            if len(similar_word_pairs['idxs']) > 0:

                pair_grads = []
                labels = []
                skips = []
                it = 0
                for i in trange(len(similar_word_pairs['idxs'])):
                    record_id = similar_word_pairs['idxs'][i]
                    record_grads = grad_data[record_id]
                    grad_left_words = [x.replace('.0', '') for x in record_grads['left_words']]
                    grad_right_words = [x.replace('.0', '') for x in record_grads['right_words']]
                    record_pairs = similar_word_pairs['pairs'][i]

                    for k in range(len(record_pairs)):
                        pair = (str(record_pairs[k][0]), str(record_pairs[k][1]))

                        if pair[0] not in grad_left_words or pair[1] not in grad_right_words:
                            skips.append(it)
                            it += 1
                            continue

                        left_word_idx = grad_left_words.index(pair[0])
                        right_word_idx = grad_right_words.index(pair[1])

                        pair_grad = record_grads['left_grads'][left_word_idx] + record_grads['right_grads'][
                            right_word_idx]
                        pair_grads.append(pair_grad)
                        labels.append(encoded_dataset[record_id][2]['labels'].item())

                        it += 1

                assert len(pair_grads) == len([sem_model_sims[x] for x in range(len(sem_model_sims)) if x not in skips])
                print(f"\tSKIPS: {len(skips)}")

                grads.append({
                    'use_case': uc,
                    'thr': thr,
                    'grads': pair_grads,
                    'tot_sim_word_pairs': np.sum([len(x) for x in similar_word_pairs['pairs']]),
                    'tot_pairs': num_all_pairs,
                    'sim_pairs_coverage': (np.sum([len(x) for x in similar_word_pairs['pairs']]) / num_all_pairs) * 100,
                    'dataset_coverage': (len(similar_word_pairs) / len(pair_of_entities)) * 100,
                    'sem_model_sims': [sem_model_sims[x] for x in range(len(sem_model_sims)) if x not in skips],
                    'labels': labels,
                    'skips': len(skips)
                })

            else:
                grads.append({
                    'use_case': uc,
                    'thr': thr,
                    'grads': None,
                    'tot_sim_word_pairs': 0,
                    'tot_pairs': num_all_pairs,
                    'sim_pairs_coverage': None,
                    'dataset_coverage': None,
                    'sem_model_sims': None,
                    'labels': None,
                    'skips': None
                })

    return grads


def load_results(res_type, res_metric):

    with open(os.path.join(RESULTS_DIR, f'bert_{res_type}_grad_{res_metric}.pkl'), 'rb') as f:
        res = pickle.load(f)

    res_tab = pd.DataFrame(res)
    res_tab['use_case'] = res_tab['use_case'].map(use_case_map)

    return res_tab


def plot_continuous_results(results, res_type, save_path=None):
    ncols = 4
    nrows = 3
    figsize = (18, 8)

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, sharey=True, sharex=True)
    axes = axes.flat

    # loop over the use cases
    for idx, use_case in enumerate(use_case_map.values()):
        ax = axes[idx]
        uc_results = results[results['use_case'] == use_case]

        if len(uc_results) == 0:
            continue

        sem_model_sims = np.array(uc_results["sem_model_sims"].values[0])
        grads = np.array(uc_results["grads"].values[0])
        labels = np.array(uc_results['labels'].values[0])
        match_labels = labels == 1
        ax.scatter(x=grads, y=sem_model_sims, alpha=1, color='tab:blue')
        # ax.scatter(x=grads[match_labels], y=sem_model_sims[match_labels], alpha=0.3, color='tab:orange')
        # ax.scatter(x=grads[~match_labels], y=sem_model_sims[~match_labels], alpha=0.3, color='tab:blue')

        ax.set_title(use_case, fontsize=16)

        if idx % ncols == 0:
            if res_type == 'syntax':
                ylabel = 'Jaccard sim.'
            elif res_type == 'semantic':
                ylabel = 'FastText cosine sim.'
            else:
                raise NotImplementedError()

            ax.set_ylabel(ylabel, fontsize=16)

        if idx // ncols == nrows - 1:
            ax.set_xlabel('Gradients', fontsize=16)
        ax.xaxis.set_tick_params(labelsize=14)
        ax.yaxis.set_tick_params(labelsize=14)
        # start, end = ax.get_ylim()
        # ax.yaxis.set_ticks(np.arange(start, end, 25))

    # handles, labels = ax.get_legend_handles_labels()
    # fig.legend(handles, labels, bbox_to_anchor=(.58, 0.01), ncol=2, fontsize=16)

    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.2)

    if save_path:
        plt.savefig(save_path, bbox_inches='tight')

    plt.show()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Comparison of BERT gradients and embedding cosine similarities for \
                                                  pairs of words')

    # General parameters
    parser.add_argument('-use_cases', '--use_cases', nargs='+', required=True, choices=DM_USE_CASES + ['all'],
                        help='the names of the datasets')
    parser.add_argument('-data_type', '--data_type', type=str, default='train', choices=['train', 'test', 'valid'],
                        help='dataset types: train, test or valid')
    parser.add_argument('-bert_model', '--bert_model', default='bert-base-uncased', type=str,
                        help='the version of the BERT model')
    parser.add_argument('-tok', '--tok', default='sent_pair', type=str, choices=['sent_pair', 'attr_pair'],
                        help='the tokenizer for the EM entries')
    parser.add_argument('-label', '--label_col', default='label', type=str,
                        help='the name of the column in the EM dataset that contains the label')
    parser.add_argument('-left', '--left_prefix', default='left_', type=str,
                        help='the prefix used to identify the columns related to the left entity')
    parser.add_argument('-right', '--right_prefix', default='right_', type=str,
                        help='the prefix used to identify the columns related to the right entity')
    parser.add_argument('-max_len', '--max_len', default=128, type=int,
                        help='the maximum BERT sequence length')
    parser.add_argument('-permute', '--permute', default=False, type=lambda x: bool(distutils.util.strtobool(x)),
                        help='boolean flag for permuting dataset attributes')
    parser.add_argument('-v', '--verbose', default=False, type=lambda x: bool(distutils.util.strtobool(x)),
                        help='boolean flag for the dataset verbose modality')
    parser.add_argument('-return_offset', '--return_offset', default=False,
                        type=lambda x: bool(distutils.util.strtobool(x)),
                        help='boolean flag for extracting EM entry word indexes')

    # Parameters for data sampling
    parser.add_argument('-sample_size', '--sample_size', type=int,
                        help='size of the sample')
    parser.add_argument('-sample_target_class', '--sample_target_class', default='both', choices=['both', 0, 1],
                        help='classes to sample: match, non-match or both')
    parser.add_argument('-sample_seeds', '--sample_seeds', nargs='+', default=[42, 42],
                        help='seeds for each class sample. <seed non match> <seed match>')
    parser.add_argument('-ft', '--fine_tune', default=True,
                        type=lambda x: bool(distutils.util.strtobool(x)),
                        help='boolean flag for selecting fine-tuned or pre-trained model')

    # Parameters for gradient computation
    parser.add_argument('-grad_text_units', '--grad_text_units', default='words', choices=['tokens', 'words', 'attrs'],
                        help='the typology of text unit where to compute gradients')
    parser.add_argument('-grad_special_tokens', '--grad_special_tokens', default=True,
                        type=lambda x: bool(distutils.util.strtobool(x)),
                        help='whether to consider or ignore special tokens in the gradient computation')

    # Parameters for embedding similarity computation
    parser.add_argument('-sim_metric', '--sim_metric', required=True, choices=['jaccard', 'edit', 'cosine'],
                        help='the similarity/distance function')
    parser.add_argument('-sim_thrs', '--sim_thrs', nargs='+', default=[0.7],
                        help='similarity threshold to reduce the exploration space')
    parser.add_argument('-sim_op_eq', '--sim_op_eq', default=False,
                        type=lambda x: bool(distutils.util.strtobool(x)),
                        help='if consider only words with the specified similarity thresholds. \
                                Otherwise the thresholds represent a lower bound for similarity functions \
                                or an upper bound for distance functions')
    parser.add_argument('-sem_embs', '--sem_embs', default=None, choices=['fasttext'],
                        help='the embedding model for measuring the semantic similarity. For fasttext download the \
                                embeddings from "https://fasttext.cc/docs/en/english-vectors.html" and save them in the data \
                                folder. Only the version "wiki-news-300d-1M" has been tested.')
    parser.add_argument('-task', '--task', default='compute', choices=['compute', 'visualize'], type=str,
                        help='task to run: 1) "compute": run the computation of the results, 2) "visualize": show \
                                pre-computed results')

    args = parser.parse_args()

    use_cases = args.use_cases
    if use_cases == ['all']:
        use_cases = DM_USE_CASES

    conf = {
        'use_case': use_cases,
        'data_type': args.data_type,
        'model_name': args.bert_model,
        'tok': args.tok,
        'label_col': args.label_col,
        'left_prefix': args.left_prefix,
        'right_prefix': args.right_prefix,
        'max_len': args.max_len,
        'permute': args.permute,
        'verbose': args.verbose,
        'return_offset': args.return_offset,
    }

    sampler_conf = {
        'size': args.sample_size,
        'target_class': args.sample_target_class,
        'seeds': args.sample_seeds,
    }

    fine_tune = args.fine_tune
    assert fine_tune is True

    grad_conf = {
        'text_unit': args.grad_text_units,
        'special_tokens': args.grad_special_tokens,
    }
    assert grad_conf['text_unit'] == 'words'
    assert grad_conf['special_tokens'] is True

    use_case_map = ConfCreator().use_case_map

    sim_metric = args.sim_metric
    sim_thrs = [float(x) for x in args.sim_thrs]
    sim_op_eq = args.sim_op_eq
    sem_embs = args.sem_embs
    task = args.task

    if sim_metric in ['jaccard', 'edit']:
        sim_type = 'syntax'

    elif sim_metric in ['cosine']:
        sim_type = 'semantic'
        if task == 'compute':
            assert sem_embs is not None

    else:
        raise NotImplementedError()

    if sim_type == 'syntax':
        sim_metric = 'jaccard'

        if task == 'compute':
            syntactic_knowledge = get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type=sim_type,
                                                               sim_metric=sim_metric, sim_thrs=sim_thrs,
                                                               sim_op_eq=sim_op_eq, continuous_res=True)
            out_fname = os.path.join(RESULTS_DIR, f'bert_{sim_type}_grad_{sim_metric}.pkl')

            with open(out_fname, 'wb') as f:
                pickle.dump(syntactic_knowledge, f)

        elif task == 'visualize':
            res = load_results(res_type=sim_type, res_metric=sim_metric)
            plot_save_path = os.path.join(RESULTS_DIR, f'PLOT_bert_{sim_type}_grad.png')
            plot_continuous_results(res, sim_type, save_path=plot_save_path)

        else:
            raise NotImplementedError()

    elif sim_type == 'semantic':
        sim_metric = 'cosine'

        if task == 'compute':
            if sem_embs == 'fasttext':

                if not os.path.exists(FAST_TEXT_PATH):
                    fasttext_repo = 'https://fasttext.cc/docs/en/english-vectors.html'
                    raise Exception(f"Download fastText embeddings from {fasttext_repo} and save them in {DATA_DIR}.")

                print("Loading fasttext embeddings...")
                sem_emb_model = gensim.models.KeyedVectors.load_word2vec_format(FAST_TEXT_PATH, binary=False,
                                                                                encoding='utf8')
                print("fasttext embeddings loaded.")

            else:
                raise NotImplementedError()

            semantic_knowledge = get_similar_word_pairs_grads(conf, sampler_conf, grad_conf, sim_type=sim_type,
                                                              sim_metric=sim_metric, sim_thrs=sim_thrs,
                                                              sim_op_eq=sim_op_eq, sem_emb_model=sem_emb_model,
                                                              continuous_res=True)
            out_fname = os.path.join(RESULTS_DIR, f'bert_{sim_type}_grad_{sim_metric}.pkl')

            with open(out_fname, 'wb') as f:
                pickle.dump(semantic_knowledge, f)

        elif task == 'visualize':
            res = load_results(res_type=sim_type, res_metric=sim_metric)
            plot_save_path = os.path.join(RESULTS_DIR, f'PLOT_bert_{sim_type}_grad.png')
            plot_continuous_results(res, sim_type, save_path=plot_save_path)

        else:
            raise NotImplementedError()

    print(":)")
